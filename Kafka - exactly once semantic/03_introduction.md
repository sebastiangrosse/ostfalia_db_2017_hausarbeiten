# Einleitung

In einem Unternehmen existieren in den meisten Fällen mehrere Applikationen, die unentwegt Daten in Form von Ereignissen (Events) produzieren. Bei diesen Events kann es sich um Log-Einträge von Webservern, Nutzeraktionen, Nachrichten an externe Systeme oder etwas anderes handeln. Diese Daten werden zwischen den Systemen im Unternehmen ausgetauscht, um beispielsweise Auswertungen durchzuführen oder damit ein anderes System auf bestimmte Ereignisse reagiert. Im einfachsten Fall existieren im Unternehmen nur eine Hand voll Systeme, die direkt untereinander kommunizieren. Diese direkten Kommunikationswege lassen sich bei wenigen Systemen noch gut organisieren. Wenn nun allerdings immer mehr Systeme dazu kommen und nahezu jedes System mit Jedem kommuniziert, erreicht man einen Punkt an dem die Kommunikationswege nicht mehr so einfach zu handhaben sind. Was passiert beispielsweise, wenn ein System eine Downtime benötigt und man erst herausfinden muss, von wie vielen Systemen dieses System entweder Daten empfängt oder an welche Systeme es Daten versendet. Mitunter kann das System auch einfach einen Ausfall haben, und die einzelnen Systeme müssen sich darum kümmern wie sie damit umgehen.

Um dieses Problem zu umgehen, implementiert man zwischen den Systemen eine eigene Anwendung, die sich um den Empfang und die Weitergabe der Nachrichten (Events) aus den einzelnen Systemen kümmert. Die Anwendung abonniert die Daten der Systeme und veröffentlicht sie für andere Systeme, die die Daten nutzen. Das ist dann ein Publish-Subscribe Messaging System. In einem Unternehmen könnten dann mehrere solcher Systeme existieren, die für die Vermittlung von Nachrichten einer bestimmten Sorte verantwortlich sind. Zum Beispiel ein System für die Vermittlung von Logs, Ein System für die Vermittlung von in Anwendungen gesammelten Metriken oder ein System für die Aufzeichnung von Nutzerverhalten. Mit eigenen Systemen für diese Nachrichtenvermittlung entsteht schnell viel doppelte Arbeit, obwohl alle Systeme im Grunde nur Nachrichten über Ereignisse von einem Sender an einen Empfänger vermitteln.

An diesem Punkt kommt Apache Kafka ins Spiel. Apache Kafka ist ein Open Source Software Projekt der Apache Foundation, das ursprünglich bei LinkedIn entwickelt wurde. Apache Kafka wird auf der eigenen Webseite als "distributed streaming platform", also eine verteilte Platform für Datenströme, bezeichnet. Es versteht darunter grob folgende Anwendungsmöglichkeiten:

* Veröffentlichen und Abonnieren eines Datenstroms (Streams) in Form von Ereignissen, ähnlich eines Publish-Subscribe Messaging Systems
* fehler-tolerante Speicherung von Datenströmen
* Verarbeitung von Einträgen in Datenströmen, sobald diese auftreten
