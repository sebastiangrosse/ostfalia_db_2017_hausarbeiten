# 1. Einleitung

Der Austausch und das Protokollieren von Daten nimmt Jahr für Jahr zu. Dienste wie Facebook, Netflix, Amazon und Google sammeln in und mit ihren verschiedenen Diensten zahlreiche Terabyte an Daten. Mehr als über 125 Millionen aktive Nutzer spielen über das Internet aufwendige Spiele, die ebenfalls Traffic generieren. Mit der Entwicklung des Internet of Things (IoT) ist es normal geworden, dass weitere Millionen von Daten generiert werden.
Für Unternehmen stellt sich die Herausforderung, wie sie große Datensätze und/oder einen unendlichen Datenstrom sinnvoll, flexibel und effizient auswerten können.  

In der folgenden Arbeit wird das Open Source Projekt Apache Beam vorgestellt, dass sich selbst wie folgt beschreibt: _"Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines."_ [**[Beam18a]**](10_Literaturverzeichnis.md)

Um Apache Beam besser zu verstehen, werden Grundlagen bezüglich des Pipelining, Stream- und Batchprocessing umrissen. Anschließend wird auf die fundamentale Struktur von Apache Beam eingegangen (What, Where, When, How) und die breiten Möglichkeiten des Beam Models (basierend auf dem Dataflow Model) vorgestellt.
Danach wird das Zusammenspiel von Beam Model, SDKs und Runnern erläutert. Anhand eines Beispiels (MinimalWordCount) werden die Begriffe wiederholt und nochmals in den Kontext gesetzt.

Apache Beam hat eine spannende Entstehungsgeschicht, die im Kapitel: "Evolution von Apache Beam" aufgeschlüßelt wird. Am Ende wird gezeigt, was die Beam Vision ist und was die Zukunft verspricht.

------------

[☜ vorheriges Kapitel](README.md)
   |   [nächstes Kapitel ☞](2_Grundlagen.md)
